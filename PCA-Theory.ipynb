{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | PCA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis in 3 Simple Steps\n",
    "\n",
    "Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a \"black box\", and we are going to unravel its internals in 3 basic steps.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n",
    "\n",
    "# PCA Vs. LDA\n",
    "\n",
    "Both Linear Discriminant Analysis (LDA) and PCA are linear transformation methods. PCA yields the directions (principal components) that maximize the variance of the data, whereas LDA also aims to find the directions that maximize the separation (or discrimination) between different classes, which can be useful in pattern classification problem (PCA \"ignores\" class labels).\n",
    "\n",
    "In other words, PCA projects the entire dataset onto a different feature (sub)space, and LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes.\n",
    "\n",
    "\n",
    "# PCA and Dimensionality Reduction\n",
    "\n",
    "Often, the desired goal is to reduce the dimensions of a dd-dimensional dataset by projecting it onto a (k)(k)-dimensional subspace (where k<dk<d) in order to increase the computational efficiency while retaining most of the information. An important question is \"what is the size of kk that represents the data 'well'?\"\n",
    "\n",
    "Later, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix. Each of those eigenvectors is associated with an eigenvalue which can be interpreted as the \"length\" or \"magnitude\" of the corresponding eigenvector. If some eigenvalues have a significantly larger magnitude than others that the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the \"less informative\" eigenpairs is reasonable.\n",
    "\n",
    "# A Summary of the PCA Approach\n",
    "\n",
    "Standardize the data.\n",
    "\n",
    "Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "\n",
    "Sort eigenvalues in descending order and choose the kk eigenvectors that correspond to the kk largest eigenvalues where kk is the number of dimensions of the new feature subspace (k≤dk≤d)/.\n",
    "\n",
    "Construct the projection matrix WW from the selected kk eigenvectors.\n",
    "\n",
    "Transform the original dataset XX via WW to obtain a kk-dimensional feature subspace YY.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Eigendecomposition - Computing Eigenvectors and Eigenvalues\n",
    "\n",
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "# Covariance Matrix\n",
    "\n",
    "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix ΣΣ, which is a d×dd×d matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n",
    "\n",
    "σjk=1/n−1∑Ni=1(xij−x¯j)(xik−x¯k).σjk=1n−1∑i=1N(xij−x¯j)(xik−x¯k).\n",
    "\n",
    "We can summarize the calculation of the covariance matrix via the following matrix equation:\n",
    "Σ=1n−1((X−x¯)T(X−x¯))Σ=1n−1((X−x¯)T(X−x¯))\n",
    "\n",
    "where x¯x¯ is the mean vector x¯=∑k=1nxi.x¯=∑k=1nxi.\n",
    "\n",
    "The mean vector is a dd-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.\n",
    "\n",
    "\n",
    "# Correlation Matrix\n",
    "\n",
    "Especially, in the field of \"Finance,\" the correlation matrix typically used instead of the covariance matrix. However, the eigendecomposition of the covariance matrix (if the input data was standardized) yields the same results as a eigendecomposition on the correlation matrix, since the correlation matrix can be understood as the normalized covariance matrix. Eigendecomposition of the standardized data based on the correlation matrix.\n",
    "\n",
    "# Singular Vector Decomposition\n",
    "\n",
    "While the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Selecting Principal Components\n",
    "\n",
    "The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1.\n",
    "\n",
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top kk eigenvectors.\n",
    "\n",
    "After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Projection Onto the New Feature Space\n",
    "In this last step we will use the 4×24×2-dimensional projection matrix WW to transform our samples onto the new subspace via the equation\n",
    "Y=X×WY=X×W, where YY is a 150×2150×2 matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
